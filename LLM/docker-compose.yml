# docker-compose -f docker-compose.yml --env-file .env --project-name llm up -d --build --no-recreate

services:
  llm:
    build: 
      context: ./llm_base_pytorch
      dockerfile: Dockerfile.pytorch
    ports:
      - "${JUPYTER_PORT}:${JUPYTER_PORT}"
      - 8501:8501
    volumes:
      - ./volumes/notebooks:${JUPYTER_DIR}
    environment:
      JUPYTER_PORT: ${JUPYTER_PORT}
      JUPYTER_DIR: ${JUPYTER_DIR}
      JUPYTER_NOTEBOOK_PASSWORD: ${JUPYTER_PASSWORD}
      JUPYTER_APP_ALLOW_ORIGIN: ${JUPYTER_APP_ALLOW_ORIGIN}
      JUPYTER_APP_IP: ${JUPYTER_APP_IP}
      PORT_RETRIES: ${PORT_RETRIES}
      JUPYTER_TOKEN: ${JUPYTER_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# # ######################### gradio#########################

  gradio:
    build:
      context: ./gradio
      dockerfile: Dockerfile.gradio
    # (만약 llm 서비스를 사용해야 한다면 depends_on 추가)
    # depends_on:
    #   - llm
    ports:
      - "7860:7860"
    volumes:
      - ./volumes/gradio:/app/gradio
    # environment:

# ##########################triton ###########################

  triton:
    build: 
      context: ./triton
      dockerfile: Dockerfile.triton
    ports:
      #TRITON
      - "${TRITON_PORT_HTTP_gRPC_API}:${TRITON_PORT_HTTP_gRPC_API}"
      - "${TRITON_PORT_gRPC_API}:${TRITON_PORT_gRPC_API}"
      - "${TRITON_PORT_Prometheus_metrics}:${TRITON_PORT_Prometheus_metrics}"
    volumes:
      - ${DOCKER_TRITON_VOLUME}

    environment:
      # TRITON
      TRITON_PORT_HTTP_gRPC_API : ${TRITON_PORT_HTTP_gRPC_API}
      TRITON_PORT_gRPC_API : ${TRITON_PORT_gRPC_API}
      TRITON_PORT_Prometheus_metrics : ${TRITON_PORT_Prometheus_metrics}

      TRITON_MODEL_ROOT_DIR : ${TRITON_MODEL_ROOT_DIR}
      TRITON_MODEL_KIND : ${TRITON_MODEL_KIND}
      TRITON_MODEL_VERSION : ${TRITON_MODEL_VERSION}
      TRITON_SELECTED_MODEL_NAME : ${TRITON_SELECTED_MODEL_NAME}
      TRITON_MAX_OUTPUT_LENGTH : ${TRITON_MAX_OUTPUT_LENGTH}

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]


############tensorrt##########################
  tensorrt:
    build: 
      context: ./tensorrt
      dockerfile: Dockerfile.tensorrt
    volumes:
      - ${DOCKER_TENSORRT_VOLUME}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

############## langchain / streamlit ##############################

  langchain:
    build:
      context: ./langchain
      dockerfile: Dockerfile.langchain
    # (만약 llm 서비스를 사용해야 한다면 depends_on 추가)
    # depends_on:
    #   - llm
    ports:
      - "7861:7861"
    volumes:
      - ./volumes/langchain:/app/langchain

  