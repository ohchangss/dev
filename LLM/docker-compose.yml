# docker-compose -f docker-compose.yml --env-file .env --project-name llm up -d --build --no-recreate

services:
  llm:
    build: 
      context: .
      dockerfile: Dockerfile.pytorch
    ports:
      - "${JUPYTER_PORT}:${JUPYTER_PORT}"
      - 8501:8501
    volumes:
      - ./volumes/notebooks:${JUPYTER_DIR}
    environment:
      JUPYTER_PORT: ${JUPYTER_PORT}
      JUPYTER_DIR: ${JUPYTER_DIR}
      JUPYTER_NOTEBOOK_PASSWORD: ${JUPYTER_PASSWORD}
      JUPYTER_APP_ALLOW_ORIGIN: ${JUPYTER_APP_ALLOW_ORIGIN}
      JUPYTER_APP_IP: ${JUPYTER_APP_IP}
      PORT_RETRIES: ${PORT_RETRIES}
      JUPYTER_TOKEN: ${JUPYTER_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# # ######################### gradio#########################

  gradio:
    build:
      context: .
      dockerfile: Dockerfile.gradio
    # (만약 llm 서비스를 사용해야 한다면 depends_on 추가)
    # depends_on:
    #   - llm
    ports:
      - "7860:7860"
    volumes:
      - ./volumes/gradio:/app/gradio
    # environment:

# ##########################triton ###########################

  triton:
    build: 
      context: .
      dockerfile: Dockerfile.triton
    ports:
      #TRITON
      - "${TRITON_PORT_HTTP_gRPC_API}:${TRITON_PORT_HTTP_gRPC_API}"
      - "${TRITON_PORT_gRPC_API}:${TRITON_PORT_gRPC_API}"
      - "${TRITON_PORT_Prometheus_metrics}:${TRITON_PORT_Prometheus_metrics}"
    volumes:
      - ${DOCKER_TRITON_VOLUME}

    environment:
      # TRITON
      TRITON_PORT_HTTP_gRPC_API : ${TRITON_PORT_HTTP_gRPC_API}
      TRITON_PORT_gRPC_API : ${TRITON_PORT_gRPC_API}
      TRITON_PORT_Prometheus_metrics : ${TRITON_PORT_Prometheus_metrics}

      TRITON_MODEL_ROOT_DIR : ${TRITON_MODEL_ROOT_DIR}
      TRITON_MODEL_KIND : ${TRITON_MODEL_KIND}
      TRITON_MODEL_VERSION : ${TRITON_MODEL_VERSION}
      TRITON_SELECTED_MODEL_NAME : ${TRITON_SELECTED_MODEL_NAME}
      TRITON_MAX_OUTPUT_LENGTH : ${TRITON_MAX_OUTPUT_LENGTH}

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]


############tensorrt##########################
  tensorrt:
    build: 
      context: .
      dockerfile: Dockerfile.tensorrt
    volumes:
      - ${DOCKER_TENSORRT_VOLUME}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

############## langchain / streamlit ##############################

  langchain:
    build:
      context: .
      dockerfile: Dockerfile.langchain
    # (만약 llm 서비스를 사용해야 한다면 depends_on 추가)
    # depends_on:
    #   - llm
    ports:
      - "7861:7861"
    volumes:
      - ./volumes/langchain:/app/langchain

############## n8n ##############################

#   postgres:
#     image: postgres:16-alpine
#     networks: ['demo']
#     restart: unless-stopped
#     environment:
#       - POSTGRES_USER
#       - POSTGRES_PASSWORD
#       - POSTGRES_DB
#     volumes:
#       - postgres_storage:/var/lib/postgresql/data
#     healthcheck:
#       test: ['CMD-SHELL', 'pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}']
#       interval: 5s
#       timeout: 5s
#       retries: 10

#   n8n-import:
#     <<: *service-n8n
#     container_name: n8n-import
#     entrypoint: /bin/sh
#     command:
#       - "-c"
#       - "n8n import:credentials --separate --input=/backup/credentials && n8n import:workflow --separate --input=/backup/workflows"
#     volumes:
#       - ./n8n/backup:/backup
#     depends_on:
#       postgres:
#         condition: service_healthy
#   n8n:
#     <<: *service-n8n
#     container_name: n8n
#     restart: unless-stopped
#     ports:
#       - 5678:5678
#     volumes:
#       - n8n_storage:/home/node/.n8n
#       - ./n8n/backup:/backup
#       - ./shared:/data/shared
#     depends_on:
#       postgres:
#         condition: service_healthy
#       n8n-import:
#         condition: service_completed_successfully

#   qdrant:
#     image: qdrant/qdrant
#     container_name: qdrant
#     networks: ['demo']
#     restart: unless-stopped
#     ports:
#       - 6333:6333
#     volumes:
#       - qdrant_storage:/qdrant/storage

#   ollama-cpu:
#     profiles: ["cpu"]
#     <<: *service-ollama

#   ollama-gpu:
#     profiles: ["gpu-nvidia"]
#     <<: *service-ollama
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: 1
#               capabilities: [gpu]

#   ollama-pull-llama-cpu:
#     profiles: ["cpu"]
#     <<: *init-ollama
#     depends_on:
#       - ollama-cpu

#   ollama-pull-llama-gpu:
#     profiles: ["gpu-nvidia"]
#     <<: *init-ollama
#     depends_on:
#       - ollama-gpu
# volumes:
#   n8n_storage:
#   postgres_storage:
#   ollama_storage:
#   qdrant_storage:

# networks:
#   demo:

# x-n8n: &service-n8n
#   image: n8nio/n8n:latest
#   networks: ['demo']
#   environment:
#     - DB_TYPE=postgresdb
#     - DB_POSTGRESDB_HOST=postgres
#     - DB_POSTGRESDB_USER=${POSTGRES_USER}
#     - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
#     - N8N_DIAGNOSTICS_ENABLED=false
#     - N8N_PERSONALIZATION_ENABLED=false
#     - N8N_ENCRYPTION_KEY
#     - N8N_USER_MANAGEMENT_JWT_SECRET
#   links:
#     - postgres

# x-ollama: &service-ollama
#   image: ollama/ollama:latest
#   container_name: ollama
#   networks: ['demo']
#   restart: unless-stopped
#   ports:
#     - 11434:11434
#   volumes:
#     - ollama_storage:/root/.ollama

# x-init-ollama: &init-ollama
#   image: ollama/ollama:latest
#   networks: ['demo']
#   container_name: ollama-pull-llama
#   volumes:
#     - ollama_storage:/root/.ollama
#   entrypoint: /bin/sh
#   command:
#     - "-c"
#     - "sleep 3; OLLAMA_HOST=ollama:11434 ollama pull llama3.2"

